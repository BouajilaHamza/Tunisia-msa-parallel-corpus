{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee04410c",
   "metadata": {},
   "source": [
    "# MSA-Tunisian Arabic Parallel Corpus Generation Pipeline\n",
    "\n",
    "This notebook implements the pipeline described in the dataset card for creating a synthetic parallel corpus between Modern Standard Arabic (MSA) and Tunisian Arabic (aeb).\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load raw Tunisian corpus from Hugging Face.\n",
    "2. Preprocess the data.\n",
    "3. Generate MSA translations using an LLM.\n",
    "4. Apply quality filtering and regeneration.\n",
    "5. Reverse pairs and finalize the dataset.\n",
    "6. Save to JSONL.\n",
    "\n",
    "**Best Practices:**\n",
    "- Run on a machine with GPU for LLM inference.\n",
    "- Monitor memory usage for large datasets.\n",
    "- Use virtual environments (e.g., conda) for dependency management.\n",
    "\n",
    "**Dataset Chosen:** AzizBelaweid/Tunisian_Language_Dataset (138k examples, Tunisian Arabic text in Arabic script, CC BY-SA 4.0 license). If you prefer another (e.g., linagora/Tunisian_Derja_Dataset), swap the dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d1ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run once; comment out after)\n",
    "# !pip install datasets transformers sentence-transformers torch numpy tqdm unicodedata langid presidio-analyzer pyarabic\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import langid\n",
    "from presidio_analyzer import AnalyzerEngine  # For PII redaction\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer  # For LLM\n",
    "from sentence_transformers import SentenceTransformer, util  # For embeddings\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Constants (as per dataset card)\n",
    "MODEL_USED = \"InceptionAI/jais-13b\"  # Or use a available HF model like 'aubmindlab/bert-base-arabertv02' for placeholders\n",
    "MAX_ATTEMPTS = 3\n",
    "QUALITY_THRESHOLD = 0.7\n",
    "SPLITS = {\"train\": 0.8, \"validation\": 0.1, \"test\": 0.1}\n",
    "\n",
    "# Example: Test imports\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626fc92",
   "metadata": {},
   "source": [
    "## Load Raw Tunisian Corpus from Hugging Face\n",
    "\n",
    "We use the `datasets` library to load the corpus. Example dataset: AzizBelaweid/Tunisian_Language_Dataset (text column with Tunisian Arabic).\n",
    "\n",
    "**Best Practices:**\n",
    "- Load only necessary splits (e.g., 'train').\n",
    "- Subset for testing (e.g., first 100 examples).\n",
    "- Inspect data: Print samples to verify language/script.\n",
    "\n",
    "**Example Output:** A Hugging Face Dataset object with 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d1be63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b311d618b6f246179418f71d25907eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da0420957ad4ffb9c1f27a431eba007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/197M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551657a6329f4f2a84d9e21fbfe48b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2748930ffb4db6bda3636382517ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/860184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tunisian texts:\n",
      "آه هاي تتفرج متكية...\n",
      "تعدالنا بوك جأنا نصف النهار يعمللنا هيا تمشيوا توا...\n",
      "حول « حدائق تونس » بمدينة إسطنبول – Al-Sabîl\n",
      "حول « حدائق تونس » بمدينة إسطنبول\n",
      "13 janvier 2019 11 fé...\n",
      "أیام الجهات/ مدن الفنون في مدینة الثقافة: ولایة توزر 12-01-2018 - مدينة الثقافة\n",
      "Anis2019-01-12T19:28...\n",
      "يقونة أتو تجي غدوة الصباح غدوة الصباح أتو تجي أتو نعمللها شوية هكة حنة كي تبدا راقدة...\n",
      "Loaded 100 examples.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (caches locally)\n",
    "dataset = load_dataset(\"hamzabouajila/tunisian-derja-unified-raw-corpus\", split=\"train\")  # Assuming 'train' split; adjust if needed\n",
    "\n",
    "# Example: Subset for testing (use full for production)\n",
    "raw_corpus = dataset.select(range(100))['text']  # List of strings; use dataset['text'] for full\n",
    "\n",
    "# Inspect examples\n",
    "print(\"Sample Tunisian texts:\")\n",
    "for text in raw_corpus[:5]:\n",
    "    print(text[:100] + \"...\")  # Truncate for display\n",
    "\n",
    "# Best Practice: Log size\n",
    "print(f\"Loaded {len(raw_corpus)} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ca8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tunisia-msa-parallel-corpus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
