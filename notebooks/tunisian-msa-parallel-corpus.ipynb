{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:17:13.817109Z",
     "iopub.status.busy": "2025-08-27T16:17:13.816794Z",
     "iopub.status.idle": "2025-08-27T16:18:43.668493Z",
     "shell.execute_reply": "2025-08-27T16:18:43.667741Z",
     "shell.execute_reply.started": "2025-08-27T16:17:13.817075Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Collecting uuid\n",
      "  Downloading uuid-1.30.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting datetime\n",
      "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.4)\n",
      "Collecting groq\n",
      "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Collecting camel-tools\n",
      "  Downloading camel_tools-1.5.6-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from camel-tools) (1.17.0)\n",
      "Collecting docopt (from camel-tools)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from camel-tools) (5.5.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.8.1)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from camel-tools) (2.14.1)\n",
      "Collecting pyrsistent (from camel-tools)\n",
      "  Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from camel-tools) (0.9.0)\n",
      "Collecting muddler (from camel-tools)\n",
      "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting camel-kenlm>=2025.4.8 (from camel-tools)\n",
      "  Downloading camel-kenlm-2025.4.8.zip (556 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.5/556.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n",
      "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading camel_tools-1.5.6-py3-none-any.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
      "Downloading pyrsistent-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: uuid, camel-kenlm, docopt\n",
      "  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for uuid: filename=uuid-1.30-py3-none-any.whl size=6477 sha256=a87760f96d5d476383e6a7d12713bd42ae10908845cc5ed95964bfeacd74922e\n",
      "  Stored in directory: /root/.cache/pip/wheels/e0/01/df/bd20df409bd81f8b99e6cd343c5f49731dc0a20eefefdafae0\n",
      "  Building wheel for camel-kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for camel-kenlm: filename=camel_kenlm-2025.4.8-cp311-cp311-linux_x86_64.whl size=3455543 sha256=dd180f1d60a95f53e65e13275650b834af9915752b97b5c0812d2c1ff3bf3079\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/b9/62/8559aee1915ae6690fcc902a972a9ba0ff46d3ee67fea2aa44\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7715fd90a9ed3d94379cb9002ff9ceff1126e2fd1f09c5a618d4255bb50f0bf0\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "Successfully built uuid camel-kenlm docopt\n",
      "Installing collected packages: uuid, docopt, camel-kenlm, zope.interface, pyrsistent, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, muddler, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, datetime, tokenizers, nvidia-cusolver-cu12, groq, transformers, camel-tools\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed camel-kenlm-2025.4.8 camel-tools-1.5.6 datetime-5.5 docopt-0.6.2 fsspec-2025.3.0 groq-0.31.0 muddler-0.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyrsistent-0.20.0 tokenizers-0.19.1 transformers-4.43.4 uuid-1.30 zope.interface-7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets tqdm pandas numpy uuid datetime requests groq transformers torch sentence-transformers groq camel-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:26:30.246413Z",
     "iopub.status.busy": "2025-08-27T16:26:30.246117Z",
     "iopub.status.idle": "2025-08-27T16:26:30.631029Z",
     "shell.execute_reply": "2025-08-27T16:26:30.630230Z",
     "shell.execute_reply.started": "2025-08-27T16:26:30.246329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "GROQ_API_KEY = user_secrets.get_secret(\"GROQ_API_KEY\")\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:26:56.704978Z",
     "iopub.status.busy": "2025-08-27T16:26:56.704703Z",
     "iopub.status.idle": "2025-08-27T16:26:57.008766Z",
     "shell.execute_reply": "2025-08-27T16:26:57.007758Z",
     "shell.execute_reply.started": "2025-08-27T16:26:56.704960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False,token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_corpus_generator.py\n",
    "# --- For Scoring (Examples - you'll need to implement these) ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import datetime\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# --- For Groq API ---\n",
    "# pip install groq\n",
    "try:\n",
    "    from groq import Groq\n",
    "    GROQ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Groq library not found. Install with 'pip install groq' if you plan to use Groq API.\")\n",
    "    GROQ_AVAILABLE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Configuration ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"hamzabouajila/tunisian-derja-unified-raw-corpus\"\n",
    "OUTPUT_FILE = \"tunisian_msa_parallel_corpus.jsonl\"\n",
    "MAX_ATTEMPTS = 3\n",
    "SCORE_THRESHOLD = 0.7 # Composite score threshold for acceptance\n",
    "USE_GROQ = True # Set to False to use local LLM function\n",
    "LOCAL_LLM_NAME = \"your_local_model_name_or_path\" # Specify if not using Groq\n",
    "GROQ_MODEL = \"llama3-70b-8192\" # Example Groq model\n",
    "# GROQ_MODEL = \"mixtral-8x7b-32768\" # Another option\n",
    "BATCH_SIZE = 10 # Process sentences in batches for efficiency (adjust as needed)\n",
    "SPLIT_RATIO = {\"train\": 0.8, \"validation\": 0.1, \"test\": 0.1} # Approximate split ratio\n",
    "# --- Initialize Groq Client (if using) ---\n",
    "if USE_GROQ and GROQ_AVAILABLE:\n",
    "    try:\n",
    "        groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "        if not groq_client.api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY environment variable not set.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Groq client: {e}\")\n",
    "        USE_GROQ = False\n",
    "elif USE_GROQ and not GROQ_AVAILABLE:\n",
    "    USE_GROQ = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Translation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_tn_to_msa_local(tn_sentence):\n",
    "    \"\"\"Placeholder for translating using a local LLM.\"\"\"\n",
    "    # Implement your local LLM translation logic here.\n",
    "    # Example using transformers pipeline:\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"ترجمة الجملة التونسية إلى العربية الفصحى: {tn_sentence}\"},\n",
    "                ]\n",
    "        result = pipe(messages, temperature=0.5)\n",
    "        return result[0]['generated_text'][-1][\"content\"].split(\"\\n\\n\")[-1].split(\"الترجمة هي:\")[-1].strip().replace(\"**\",\"\")\n",
    "    # .split(\"MSA Translation:\")[-1].strip()\n",
    "    except Exception as e:\n",
    "         print(f\"Error translating '{tn_sentence}' with local LLM: {e}\")\n",
    "         return None\n",
    "    #Example using llama-cpp-python (requires setup):\n",
    "    # from llama_cpp import Llama\n",
    "    # llm = Llama(model_path=\"path/to/your/gguf/model\")\n",
    "    # prompt = f\"Translate the following Tunisian Arabic text to Modern Standard Arabic:\\n\\n{tn_sentence}\\n\\nMSA Translation:\"\n",
    "    # output = llm(prompt, max_tokens=512, stop=[\"\\n\"], echo=True)\n",
    "    # return output['choices'][0]['text'].split(\"MSA Translation:\")[-1].strip()\n",
    "\n",
    "    # Placeholder return\n",
    "    # print(f\"Local LLM translation not implemented for: {tn_sentence}\")\n",
    "    # return f\"[LOCAL_LLM_TRANSLATION_PLACEHOLDER] {tn_sentence}\" # Indicate it's a placeholder\n",
    "\n",
    "\n",
    "# --- Translation Functions ---\n",
    "def translate_tn_to_msa_groq(tn_sentence):\n",
    "    \"\"\"Translate Tunisian Arabic to MSA using Groq API.\"\"\"\n",
    "    if not USE_GROQ or not GROQ_AVAILABLE:\n",
    "        raise ValueError(\"Groq API not configured or available.\")\n",
    "    try:\n",
    "        prompt = f\"Translate the following Tunisian Arabic text to Modern Standard Arabic:\\n\\n{tn_sentence}\\n\\nMSA Translation:\"\n",
    "        chat_completion = groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            model=GROQ_MODEL,\n",
    "            temperature=0.2, # Lower temperature for more deterministic output\n",
    "            max_tokens=512, # Adjust based on expected sentence length\n",
    "            top_p=1,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "        )\n",
    "        msa_translation = chat_completion.choices[0].message.content.strip()\n",
    "        return msa_translation\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating '{tn_sentence}' with Groq: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translate_tn_to_msa(tn_sentence):\n",
    "    \"\"\"Wrapper to select translation method.\"\"\"\n",
    "    if USE_GROQ and GROQ_AVAILABLE:\n",
    "        return translate_tn_to_msa_groq(tn_sentence)\n",
    "    else:\n",
    "        return translate_tn_to_msa_local(tn_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Tunisian Arabic text \"شنوة أحوالك؟\" can be translated to Modern Standard Arabic as:\\n\\nكيف أحوالك؟\\n\\n(Kayf aḥwāluk?)\\n\\nWhich means \"How are you?\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_tn_to_msa(\"شنوة أحوالك؟\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    \"\"\"Basic preprocessing for Tunisian Arabic text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    # Basic cleaning: strip whitespace, remove extra newlines\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text) # Replace multiple whitespaces with single space\n",
    "    # Add more sophisticated cleaning if needed (e.g., normalize specific characters)\n",
    "    # Filter out very short sentences or sentences with only numbers/punctuation?\n",
    "    if len(text) < 5: # Example filter\n",
    "        return None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_valid_tunisian_arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model_di = pipeline('text-classification', model='Ammar-alhaj-ali/arabic-MARBERT-dialect-identification-city')\n",
    "\n",
    "\n",
    "def is_valid_tunisian_arabic(text):\n",
    "    \"\"\"\n",
    "    Determines if the input text is Tunisian Arabic using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the text is identified as Tunisian Arabic, False otherwise.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        predictions = model_di([text])\n",
    "        if not predictions or 'label' not in predictions[0]:\n",
    "            return False\n",
    "        predictions = predictions[0]['label']\n",
    "        return predictions in [\"Tunis\",\"Sfax\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Handle any potential errors during prediction\n",
    "        print(f\"Error in dialect identification: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predincting أنا راجل تونسي\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_tunisian_arabic( \"أنا راجل تونسي\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_semantic_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "# Load the pre-trained Arabic embedding model once, outside the function, for efficiency.\n",
    "# 'UBC-NLP/MARBERT' or 'UBC-NLP/AraT5-base' are strong choices for Arabic.\n",
    "# As per the search results, newer models like the Arabic Matryoshka Embedding Models \n",
    "# or OMER NACAR models are also excellent options.\n",
    "model = SentenceTransformer('aubmindlab/bert-base-arabertv02') # Replace with your chosen model\n",
    "\n",
    "\n",
    "\n",
    "def calculate_semantic_similarity(tn_sentence, msa_candidate, model=model):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between the embeddings of a Tunisian Arabic sentence \n",
    "    and an MSA candidate sentence.\n",
    "\n",
    "    Args:\n",
    "        tn_sentence (str): The Tunisian Arabic sentence.\n",
    "        msa_candidate (str): The Modern Standard Arabic sentence.\n",
    "        model: A pre-trained SentenceTransformer model.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity score between 0.0 (no similarity) and 1.0 (identical).\n",
    "    \"\"\"\n",
    "    if not tn_sentence or not msa_candidate or not isinstance(tn_sentence, str) or not isinstance(msa_candidate, str):\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        # Encode both sentences into dense vector embeddings.\n",
    "        embeddings = model.encode([tn_sentence, msa_candidate])\n",
    "        \n",
    "        # Extract the embeddings for each sentence.\n",
    "        emb1 = embeddings[0]  # Embedding for Tunisian sentence\n",
    "        emb2 = embeddings[1]  # Embedding for MSA sentence\n",
    "\n",
    "        # Calculate cosine similarity.\n",
    "        cos_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        \n",
    "        # Ensure the result is a float and clip to valid range due to potential floating-point errors.\n",
    "        return float(np.clip(cos_sim, 0.0, 1.0))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating semantic similarity: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_semantic_similarity(\"أنا راجل تونسي\", \"أنا رجل تونسي\",model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_backtranslation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_backtranslation_score(original_tn, backtranslated_tn, model=None):\n",
    "    \"\"\"Calculate similarity between original TN and back-translated TN.\"\"\"\n",
    "    # Requires a TN -> MSA -> TN pipeline or a dedicated TN-MSA-TN model\n",
    "    # Example similarity calculation (e.g., using embedding similarity again)\n",
    "    try:\n",
    "        similarity = calculate_semantic_similarity(original_tn, backtranslated_tn, model=model) # Reuse semantic sim function\n",
    "        return similarity\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating backtranslation score: {e}\")\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_backtranslation_score(\"أنا راجل تونسي\", \"أنا رجل تونسي\",model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate_lm_fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Load the tokenizer and model once, outside the function, for efficiency.\n",
    "# 'aubmindlab/aragpt2-base' is a good choice for Modern Standard Arabic.\n",
    "# Other models like 'marefa-nlp/ajeeb-gpt2-large-ar' or 'CAMeL-Lab/bert-base-arabic' \n",
    "# (adapted for sequence scoring) are also available.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('aubmindlab/aragpt2-large', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('aubmindlab/aragpt2-large', trust_remote_code=True)\n",
    "\n",
    "def calculate_lm_fluency(msa_candidate, model=model, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate fluency score for an MSA sentence using a pre-trained Arabic Language Model.\n",
    "    The score is the negative average log-likelihood. Lower perplexity (higher log-likelihood) \n",
    "    indicates higher fluency.\n",
    "    \"\"\"\n",
    "    if not msa_candidate or not isinstance(msa_candidate, str):\n",
    "        return float('-inf')\n",
    "\n",
    "    try:\n",
    "        # Tokenize the input text.\n",
    "        inputs = tokenizer(msa_candidate, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculation for inference.\n",
    "            # Get the model's outputs. Pass the input_ids as labels to compute the loss.\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            # The loss is the cross-entropy loss, which is the average negative log-likelihood.\n",
    "            avg_log_likelihood = -outputs.loss.item() # Use .item() to get a Python float\n",
    "\n",
    "        return avg_log_likelihood\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating LM fluency: {e}\")\n",
    "        return float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.509749412536621"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_lm_fluency(\"أنا رجل تونسي\",model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-27T16:32:04.853175Z",
     "iopub.status.busy": "2025-08-27T16:32:04.852903Z",
     "iopub.status.idle": "2025-08-27T16:32:08.863921Z",
     "shell.execute_reply": "2025-08-27T16:32:08.862641Z",
     "shell.execute_reply.started": "2025-08-27T16:32:04.853156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_ensemble_agreement(msa_candidates, model=None):\n",
    "    \"\"\"Calculate agreement among multiple MSA candidates.\"\"\"\n",
    "    # Example: Generate N candidates (e.g., with different sampling params)\n",
    "    # Calculate pairwise similarities and average them\n",
    "    if len(msa_candidates) < 2:\n",
    "        return 1.0 # Perfect agreement if only one\n",
    "    try:\n",
    "        similarities = []\n",
    "        for i in range(len(msa_candidates)):\n",
    "            for j in range(i+1, len(msa_candidates)):\n",
    "                sim = calculate_semantic_similarity(msa_candidates[i], msa_candidates[j], model=model)\n",
    "                similarities.append(sim)\n",
    "        avg_agreement = np.mean(similarities) if similarities else 0.0\n",
    "        return float(avg_agreement)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating ensemble agreement: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_composite_score(scores_dict):\n",
    "    \"\"\"Calculate the final composite score based on individual scores.\"\"\"\n",
    "    # Define weights for each component (you can adjust these)\n",
    "    w_semantic = 0.3\n",
    "    w_fluency = 0.2\n",
    "    w_backtrans = 0.3\n",
    "    w_ensemble = 0.2\n",
    "\n",
    "    # Normalize scores if needed (especially logprob)\n",
    "    semantic_score = scores_dict.get('semantic_similarity', 0.0)\n",
    "    fluency_score = scores_dict.get('lm_logprob', float('-inf'))\n",
    "    backtrans_score = scores_dict.get('backtranslation_score', 0.0)\n",
    "    ensemble_score = scores_dict.get('ensemble_agreement', 0.0)\n",
    "\n",
    "    # Simple normalization for logprob (example - adjust based on your model's range)\n",
    "    # Assuming logprob is negative, higher (closer to 0) is better\n",
    "    normalized_fluency = 1.0 / (1.0 + np.exp(-fluency_score)) if fluency_score != float('-inf') else 0.0\n",
    "\n",
    "    composite = (\n",
    "        w_semantic * semantic_score +\n",
    "        w_fluency * normalized_fluency +\n",
    "        w_backtrans * backtrans_score +\n",
    "        w_ensemble * ensemble_score\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error calculating LM fluency: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-270m-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:32:24.380007Z",
     "iopub.status.busy": "2025-08-27T16:32:24.379749Z",
     "iopub.status.idle": "2025-08-27T16:32:24.393875Z",
     "shell.execute_reply": "2025-08-27T16:32:24.393087Z",
     "shell.execute_reply.started": "2025-08-27T16:32:24.379992Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Main Pipeline ---\n",
    "def generate_parallel_corpus():\n",
    "    \"\"\"Main function to run the corpus generation pipeline.\"\"\"\n",
    "    print(\"Loading raw Tunisian corpus...\")\n",
    "    try:\n",
    "        raw_dataset = load_dataset(DATASET_NAME, split='train') # Assuming 'train' split contains the data\n",
    "        print(f\"Loaded {len(raw_dataset)} examples from {DATASET_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {DATASET_NAME}: {e}\")\n",
    "        return\n",
    "\n",
    "    processed_data = []\n",
    "    print(\"Starting translation and scoring pipeline...\")\n",
    "    for i, example in enumerate(tqdm(raw_dataset, desc=\"Processing\")):\n",
    "        raw_tn_text = example.get('text', example.get('sentence', None)) # Adjust key based on dataset structure\n",
    "        if not raw_tn_text:\n",
    "            continue\n",
    "\n",
    "        processed_tn = preprocess_sentence(raw_tn_text)\n",
    "        if not processed_tn or not is_valid_tunisian_arabic(processed_tn):\n",
    "            continue # Skip invalid or filtered sentences\n",
    "\n",
    "        best_msa_candidate = None\n",
    "        best_scores = {}\n",
    "        final_composite_score = 0.0\n",
    "        accepted = False\n",
    "        attempts = 0\n",
    "        all_candidates = [] # For ensemble agreement\n",
    "\n",
    "        while attempts < MAX_ATTEMPTS and not accepted:\n",
    "            attempts += 1\n",
    "            msa_candidate = translate_tn_to_msa(processed_tn)\n",
    "            if not msa_candidate:\n",
    "                continue # Skip if translation failed\n",
    "\n",
    "            all_candidates.append(msa_candidate)\n",
    "\n",
    "            # --- Scoring (Replace with actual model calls) ---\n",
    "            scores = {}\n",
    "            #scores['semantic_similarity'] = calculate_semantic_similarity(processed_tn, msa_candidate) # Pass model if needed\n",
    "            #scores['lm_logprob'] = calculate_lm_fluency(msa_candidate) # Pass model if needed\n",
    "            # Placeholder for backtranslation - would require another LLM call TN -> MSA -> TN\n",
    "            #scores['backtranslation_score'] = calculate_backtranslation_score(processed_tn, f\"[BACKTRANSLATED_{processed_tn}]\") # Placeholder\n",
    "            # Placeholder for ensemble - would require generating multiple candidates\n",
    "            #scores['ensemble_agreement'] = calculate_ensemble_agreement([msa_candidate, f\"[CANDIDATE_2_{processed_tn}]\", f\"[CANDIDATE_3_{processed_tn}]\"]) # Placeholder\n",
    "\n",
    "            #composite_score = calculate_composite_score(scores)\n",
    "\n",
    "            #if composite_score > final_composite_score:\n",
    "             #   final_composite_score = composite_score\n",
    "              #  best_msa_candidate = msa_candidate\n",
    "               # best_scores = scores.copy() # Store the best scores\n",
    "\n",
    "            #if composite_score >= SCORE_THRESHOLD:\n",
    "            #    accepted = True\n",
    "            #    break # Acceptable candidate found\n",
    "\n",
    "        # If no acceptable candidate found after MAX_ATTEMPTS, use the best one\n",
    "        #if not accepted and best_msa_candidate:\n",
    "         #   accepted = False # Explicitly mark as not accepted if below threshold\n",
    "            # You might choose to include low-scoring examples with accepted=False\n",
    "            # For now, let's include the best attempt even if below threshold\n",
    "            # If you want to discard them, add a condition here.\n",
    "\n",
    "        # --- Prepare final data record ---\n",
    "        record = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"source\": best_msa_candidate if best_msa_candidate else \"\", # MSA as source\n",
    "            \"target\": processed_tn, # TN as target\n",
    "            \"source_dialect\": raw_tn_text, # Original raw TN text\n",
    "            \"msa_generated\": best_msa_candidate if best_msa_candidate else \"\", # Initial (best) MSA candidate\n",
    "            \"score_composite\": final_composite_score,\n",
    "            \"cosine_similarity\": best_scores.get('semantic_similarity', 0.0),\n",
    "            \"lm_logprob\": best_scores.get('lm_logprob', float('-inf')),\n",
    "            \"backtranslation_score\": best_scores.get('backtranslation_score', 0.0),\n",
    "            \"ensemble_agreement\": best_scores.get('ensemble_agreement', 0.0),\n",
    "            \"num_attempts\": attempts,\n",
    "            \"accepted\": accepted,\n",
    "            \"split\": \"train\", # Placeholder, will assign splits later\n",
    "            \"date_generated\": datetime.datetime.utcnow().isoformat() + 'Z',\n",
    "            \"model_used\": GROQ_MODEL if USE_GROQ else LOCAL_LLM_NAME\n",
    "        }\n",
    "        processed_data.append(record)\n",
    "\n",
    "        # Optional: Save periodically to avoid losing data\n",
    "        if (i + 1) % 100 == 0:\n",
    "             print(f\"Processed {i+1} examples. Saving checkpoint...\")\n",
    "             # Save checkpoint logic here if needed (e.g., save every 100 examples)\n",
    "\n",
    "    print(f\"Pipeline completed. Total processed examples: {len(processed_data)}\")\n",
    "\n",
    "    # --- Assign Splits ---\n",
    "    print(\"Assigning dataset splits...\")\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    if not df.empty:\n",
    "        # Shuffle the dataframe\n",
    "        df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        total_size = len(df_shuffled)\n",
    "        train_end = int(SPLIT_RATIO['train'] * total_size)\n",
    "        val_end = train_end + int(SPLIT_RATIO['validation'] * total_size)\n",
    "\n",
    "        df_shuffled.loc[:train_end, 'split'] = 'train'\n",
    "        df_shuffled.loc[train_end:val_end, 'split'] = 'validation'\n",
    "        df_shuffled.loc[val_end:, 'split'] = 'test'\n",
    "\n",
    "        # Convert back to list of dicts\n",
    "        processed_data = df_shuffled.to_dict('records')\n",
    "    else:\n",
    "         print(\"No data to assign splits to.\")\n",
    "\n",
    "    # --- Save to JSONL ---\n",
    "    print(f\"Saving final dataset to {OUTPUT_FILE}...\")\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            for record in processed_data:\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "        print(\"Dataset saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generate_parallel_corpus()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
